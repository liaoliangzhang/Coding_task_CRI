---
title: "Tech 1, Data Prep: THE CROWDSOURCED REPLICATION INITIATIVE (CRI)"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

Principal Investigators:
   Nate Breznau
   
   Eike Mark Rinke
   
   Alexander Wuttke

Code Compiled by 
   Nate Breznau, breznau.nate@gmail.com
   
   Hung Nguyen, hunghvnguyen@gmail.com
   

```{r setup, include=FALSE}
rm(list = ls())
library(pacman)

pacman::p_load("readstata13","dplyr","readr","lattice","tidyr","readxl","knitr","boot","ragg","kableExtra")

#Column no missing function
completeFun <- function(data, desiredCols) {
  completeVec <- complete.cases(data[, desiredCols])
  return(data[completeVec, ])
}
```


## Data Prep


### CRI Margins & Model Specs

CRI Model Specifications and Margins in the file "CRI Model Specifications and Margins.xlsx". This model specification file was qualitatively coded by hand from each team's code and the margins came from CRI_Expansion_All.Rmd.

We set up two versions here: *cri* and *cri_str*. One is for dichotomous numerical values and the other is for string names of each variable (good for spec curve plotting).



```{r load}
# Data generated from qualitative model specification coding
cri_str <- read_excel("data/CRI Model Specifications and Margins.xlsx", sheet = "Data")

# replace p values of 0.000000 with 0.00001
cri_str <- cri_str %>%
  mutate(p = ifelse(p < 0.00001, 0.00001,p))
```

### Standardization

We face a problem that a one percent change in net migration (within-country) is huge relative to a 1% difference in percent foreign-born across countries.

Standardization will produce some extreme outliers, so a 'better' strategy is to adjust the sd of Flow to be equal to that of Stock. However, there are some wide outliers in stock that bunches up the dispersion of flow, therefore we need the sd without these extreme outliers

```{r standardizeDV}
# Team 104 is massive outlier, we have to rescale the estimates
cri_str <- cri_str %>%
  mutate(AME = ifelse(id == "t104m2" | id == "t104m4", AME/10, AME),
         lower = ifelse(id == "t104m2" | id == "t104m4", lower/10, lower),
         upper = ifelse(id == "t104m2" | id == "t104m4", upper/10, upper))

cri_str <- cri_str %>%
  mutate(AMEssd = ifelse(main_IV_type == "Stock", AME, NA),
         AMEfsd = ifelse(main_IV_type == "Flow", AME, NA),
         AMEssd = sd(AMEssd, na.rm = T),
         AMEfsd = sd(AMEfsd, na.rm = T),
         AME_Z = AME*(AMEssd/AMEfsd),
         lower_Z = lower*(AMEssd/AMEfsd),
         upper_Z = upper*(AMEssd/AMEfsd),
         AME_Z = ifelse(main_IV_type == "Flow", AME_Z, AME),
         lower_Z = ifelse(main_IV_type == "Flow", lower_Z, lower),
         upper_Z = ifelse(main_IV_type == "Flow", upper_Z, upper))

cri_str <- select(cri_str, -c(AMEssd,AMEfsd))

```


```{r recodes}
cri_str <- cri_str %>%
  mutate(Hresult = ifelse(Hnotest==1, "No test", ifelse(Hsupport==1, "Support", ifelse(Hreject==1, "Reject", ifelse(Hmixed==1 & main_IV_type == "Stock" & Hnotest_stock==1, "No test", ifelse(Hmixed==1 & main_IV_type == "Stock" & Hsupport_stock==1, "Support", ifelse(Hmixed==1 & main_IV_type == "Stock" & Hreject_stock==1, "Reject", ifelse(Hmixed==1 & main_IV_type == "Flow" & Hnotest_net==1, "No test", ifelse(Hmixed==1 & main_IV_type == "Flow" & Hsupport_net==1, "Support", ifelse(Hmixed==1 & main_IV_type == "Flow" & Hreject_net==1, "Reject", ifelse(Hmixed==1 & main_IV_type == "Change in Flow" & Hnotest_net==1, "No test", ifelse(Hmixed==1 & main_IV_type == "Change in Flow" & Hsupport_net==1, "Support", ifelse(Hmixed==1 & main_IV_type == "Change in Flow" & Hreject_net==1, "Reject", NA)))))))))))),
         Hsup = ifelse(Hresult=="Support",1,0),
         Hrej = ifelse(Hresult=="Reject",1,0),
         Hno = ifelse((Hresult == "Support" | Hresult == "Reject"), 0, 1),
         HresultF = as.factor(Hresult),
         AME_sup_p05 = ifelse(AME < 0 & p < 0.05, 1, 0),
         AME_sup_p10 = ifelse(AME < 0 & p < 0.1, 1, 0),
         countries = as.numeric(num_countries)
         )

cri_str <- select(cri_str, -c(num_countries))




# easier country names (for string version)
cri_str <- cri_str %>%
  dplyr::rename(
    AU = australia  ,
    AT = austria  ,
    BE = belgium	 ,
    BG = bulgaria	 ,
    CA = canada	 ,
    CL = chile	 ,
    HR = croatia	 ,
    CY = cyprus	 ,
    CZ = czechia	 ,
    DK = denmark	 ,
    FI = finland	 ,
    FR = france	 ,
    DE = germany	 ,
    HU = hungary	 ,
    IS = iceland	 ,
    IN = india	 ,
    IE = ireland	 ,
    IL = israel	 ,
    IT = italy	 ,
    JP = japan	 ,
    KR = korea	 ,
    LV = latvia  ,
    LT = lithuania	 ,
    NT = netherlands  ,
    NZ = new_zealand	 ,
    NO = norway  ,
    PH = philippines  ,
    PL = poland	 ,
    PT = portugal	 ,
    RU = russia	 ,
    SK = slovakia	 ,
    SI = slovenia	 ,
    ES = spain	 ,
    SE = sweden  ,
    CH = switzerland  ,
    UK = great_britain  ,
    US = usa	 ,
    ZA = south_africa  ,
    TW = taiwan  ,
    TR = turkey  ,
    UY = uruguay	 ,
    VE = venezuela  
  )

# preserving the order of teams and models
cri_str <- cri_str[order(cri_str$count),]

# make two datasets, one numeric, one string
cri <- cri_str
```

```{r string_encode}
#replace all 1's with the column name for all variable
w <- select(cri_str, Jobs:anynonlin, id)

for (i in 1:length(w)) {
    w[[i]] <- ifelse(w[[i]]=="1", colnames(w)[i], w[[i]])
}

# country divisions not used
w <- select(w, -c(germany_west, germany_east, n_ireland))
cri <- select(cri, -c(germany_west, germany_east, n_ireland))

crispeca <- select(cri_str, -c(Jobs:anynonlin))
cri_str <- merge(crispeca, w , by = "id")

cri_str <- select(cri_str, Jobs:anynonlin, everything())

# replace 0's with NA
cri_str[c(1:161)][cri_str[c(1:161)] == 0] <- NA
```

### Categorical Variables

We take the dichotomous variables and merge them into categorical variables for each type of model component.

```{r cat_vars}

cri_str <- cri_str %>%
  mutate(iv_type = coalesce(Stock,Flow,ChangeFlow), # main test var
         mator = coalesce(ols,logit,bayes,ologit,mlogit,ml_glm), # Estimator
         software = coalesce(stata,r,mplus,mlwin,spss), # Software
         indepv = coalesce(socx_ivC, unemprate_ivC, emplrate_ivC, gdp_ivC) # main country-level IVs
         )

# send to numerical data for use as factor variables

cri_strT <- select(cri_str, id, iv_type, mator, software, indepv)

cri <- left_join(cri, cri_strT, by = "id")

rm(crispeca,w)

# use numerical data to generate unidimensional strings
criT <- cri %>%
      mutate(twowayfe = ifelse(twowayfe == 1,"Yes","No"),
      cluster_any = ifelse(cluster_any == 1,"Yes","No"),
      mlm_re = ifelse(mlm_re == 1,"Yes","No"),
      mlm_fe = ifelse(mlm_fe == 1,"Yes","No"),
      logit = ifelse(logit == 1,"Yes","No"),
      dichotomize = ifelse(dichotomize == 1,"dichotomous","continuous")
      )

criT <- select(criT, id, twowayfe, cluster_any, mlm_re, mlm_fe, logit, dichotomize)
cri_str <- select(cri_str, -c(twowayfe, cluster_any, mlm_re, mlm_fe, logit, dichotomize))
cri_str <- left_join(cri_str, criT, by = "id")

rm(criT, cri_strT)

# combine all different scale types, plus rmv extra space in Jobs
cri_str$dv_type <- cri_str$DV
cri_str$dv_type <- car::recode(cri_str$dv_type, 
                              "c('Scale_6', 'Scale_4', 'Scale_5', 'Scale_Desrv', 'Scale_Univ') = 'Scale';
                              'Jobs ' = 'Jobs'")

# Better labeling
cri_str$indepv <- car::recode(cri_str$indepv,
                             "'socx_ivC' = 'Soc_Spending'; 
                             'unemprate_ivC' = 'Unemp_Rate';
                             'emplrate_ivC' = 'Emp_Rate';
                             'gdp_ivC' = 'GDP_Per_Capita'")

cri_str$indepv[is.na(cri_str$indepv)] <- 'None'

cri_str$software <- car::recode(cri_str$software,
                             "'stata' = 'Stata';
                             'r' = 'R';
                             'spss' = 'SPSS';
                             'mplus' = 'Mplus';
                             'mlwin' = 'MLwiN'")

```
### Merge in UniPark Survey

Original work up of individual/team researcher qualities done in Stata

NOTE: These Stata files include identifying information breaking the anonymity of the teams, so we have to clean them before making them public. We may not do this (what do we gain?) so therefore, we have provided a file with the cleaned and merged data "cri_combined_recoded_long_nolab.dta"

This was created from the following:

Required code files:
  master.do
      convert spss.do
      merge_waves.do
      recode.do
  Cri_spec.do

Required data files:
  W1_export.sps
  W2_export.sps
  W3_export.sps
  W4_export.sps
  
Note that teams are now in wide format, so that each variable has up to three values, one for each team member. Also, each result is attached to each team member, thus there are N-results repeated observations of team members.



```{r combine }
# original survey data from Unipark Survey SPSS files
cri_part <- readstata13::read.dta13("data/cri_combined_recoded.dta")

# datafile merged into within-team wide format

cri2 <- readstata13::read.dta13("data/cri_combined_recoded_long_nolab.dta")

cri <- full_join(cri, cri2, by = "u_teamid")

# remove teams that dropped out
# Column no missing function
cri <- completeFun(cri, "id")

cri <- select(cri, u_teamid, id, everything())


```


```{r codebook, include=F, warning=F, message=F}
# Note that at the end of each variable we add a 1, 2 and 3 for up to three team member's responses

# These are the labels for the unipark variables

#Statistical knowledge and experience

# v_100 #Not enough methods skills, 1-did not constrain me
# v_101 #Not enough software programming skills, 1-did not constrain me
# v_34 #Difficulty of replication, 1-most difficult
# backgr_exp_teach_stat #"1 How many stats/quant methods courses taught?"
# backgr_exp_famil_mlm #"1 Familiarity with multilevel modeling (5=very familiar)"
# v_18 #"1 Published on statistics/methods?"
# v_21 #"1 Published using multilevel regression?"
# 
# #Topical knowledge and experience
# 
# v_17 #"1 Published on immigration?"
# v_19 #"1 Published on public policy/welf state?"
# v_20 #"1 Published on policy prefs/public opinion?"
# v_88  #I was very interested in the substantive topic.
# v_35 #"1 Not at all familiar with the lilterature on immigration/policy"
# v_36 #"1 Read some of the literature on imm/policy" (vague in-between category, drop)
# v_37 #"1 Read much of the literature on imm/policy"
# v_38 #"1 Published articles or books on imm/policy"
# v_39 #"1 Taught courses on imm/policy"
# v_40 #"1 Often discuss imm/policy with colleagues"
# 
# belief_H1_1 #Belief in hypothesis
# belief_agecare_1
# belief_unempl_1
# belief_income_1
# belief_housing_1
# belief_labour_1
# belief_health_1
# 
# 
# label var v_171 "1 Published on immigration?"
# label var v_181 "1 Published on statistics/methods?"
# label var v_191 "1 Published on public policy/welf state?"
# label var v_201 "1 Published on policy prefs/public opinion?"
# label var v_211 "1 Published using multilevel regression?"
# label var v_172 "2 Published on immigration?"
# label var v_182 "2 Published on statistics/methods?"
# label var v_192 "2 Published on public policy/welf state?"
# label var v_202 "2 Published on policy prefs/public opinion?"
# label var v_212 "2 Published using multilevel regression?"
# label var v_173 "3 Published on immigration?"
# label var v_183 "3 Published on statistics/methods?"
# label var v_193 "3 Published on public policy/welf state?"
# label var v_203 "3 Published on policy prefs/public opinion?"
# label var v_213 "3 Published using multilevel regression?"
# 
# label var backgr_exp_teach_stat1 "1 How many stats/quant methods courses taught?"
# label var backgr_exp_famil_mlm1 "1 Familiarity with multilevel modeling (5=very familiar)"
# label var belief_H1_11 "1 Immigration reduces/increase support for social policy (1=strongly reduces)"
# label var backgr_exp_teach_stat2 "2 How many stats/quant methods courses taught?"
# label var backgr_exp_famil_mlm2 "2 Familiarity with multilevel modeling (5=very familiar)"
# label var belief_H1_12 "2 Immigration reduces/increase support for social policy (1=strongly reduces)"
# label var backgr_exp_teach_stat3 "3 How many stats/quant methods courses taught?"
# label var backgr_exp_famil_mlm3 "3 Familiarity with multilevel modeling (5=very familiar)"
# label var belief_H1_13 "3 Immigration reduces/increase support for social policy (1=strongly reduces)"
# 
# label var v_331 "1 Individual time spent on replication"
# label var v_341 "1 Difficulty of replication (1=most difficult)"
# label var v_351 "1 Not at all familiar with the lilterature on immigration/policy"
# label var v_361 "1 Read some of the literature on imm/policy"
# label var v_371 "1 Read much of the literature on imm/policy"
# label var v_381 "1 Published articles or books on imm/policy"
# label var v_391 "1 Taught courses on imm/policy"
# label var v_401 "1 Often discuss imm/policy with colleagues"
# label var v_411 "1 Enjoyment of replication (1=extremely fun)"
# label var v_431 "1 Convincingness of Brady & Finnigan tests(1=most convincing)"
# label var v_332 "2 Individual time spent on replication"
# label var v_342 "2 Difficulty of replication (1=most difficult)"
# label var v_352 "2 Not at all familiar with the lilterature on immigration/policy"
# label var v_362 "2 Read some of the literature on imm/policy"
# label var v_372 "2 Read much of the literature on imm/policy"
# label var v_382 "2 Published articles or books on imm/policy"
# label var v_392 "2 Taught courses on imm/policy"
# label var v_402 "2 Often discuss imm/policy with colleagues"
# label var v_412 "2 Enjoyment of replication (1=extremely fun)"
# label var v_432 "2 Convincingness of Brady & Finnigan tests(1=most convincing)"
# label var v_333 "3 Individual time spent on replication"
# label var v_343 "3 Difficulty of replication (1=most difficult)"
# label var v_353 "3 Not at all familiar with the lilterature on immigration/policy"
# label var v_363 "3 Read some of the literature on imm/policy"
# label var v_373 "3 Read much of the literature on imm/policy"
# label var v_383 "3 Published articles or books on imm/policy"
# label var v_393 "3 Taught courses on imm/policy"
# label var v_403 "3 Often discuss imm/policy with colleagues"
# label var v_413 "3 Enjoyment of replication (1=extremely fun)"
# label var v_433 "3 Convincingness of Brady & Finnigan tests(1=most convincing)"
# 
# -> tabulation of v_88  
# 
#            I was very interested in the |
#                      substantive topic. |      Freq.     Percent        Cum.
# ----------------------------------------+-----------------------------------
# Reason does not apply to me apply at al |         10        6.33        6.33
#           Reason applies to me a little |         30       18.99       25.32
#                             Neither nor |         22       13.92       39.24
#           Reason somewhat applies to me |         51       32.28       71.52
#              Reason applies to me a lot |         45       28.48      100.00
# ----------------------------------------+-----------------------------------
#                                   Total |        158      100.00
# 
# -> tabulation of v_89  
# 
# Colleagues asked me to join their team. |      Freq.     Percent        Cum.
# ----------------------------------------+-----------------------------------
#                                       0 |          2        1.27        1.27
# Reason does not apply to me apply at al |         63       39.87       41.14
#           Reason applies to me a little |         15        9.49       50.63
#                             Neither nor |          4        2.53       53.16
#           Reason somewhat applies to me |         33       20.89       74.05
#              Reason applies to me a lot |         41       25.95      100.00
# ----------------------------------------+-----------------------------------
#                                   Total |        158      100.00
# 
# -> tabulation of v_90  
# 
#            The prospect of a scientific |
#              publication was appealing. |      Freq.     Percent        Cum.
# ----------------------------------------+-----------------------------------
#                                       0 |          1        0.63        0.63
# Reason does not apply to me apply at al |         10        6.33        6.96
#           Reason applies to me a little |         19       12.03       18.99
#                             Neither nor |         24       15.19       34.18
#           Reason somewhat applies to me |         66       41.77       75.95
#              Reason applies to me a lot |         38       24.05      100.00
# ----------------------------------------+-----------------------------------
#                                   Total |        158      100.00
# 
# -> tabulation of v_91  
# 
#         I expected the project to be an |
#                   enjoyable experience. |      Freq.     Percent        Cum.
# ----------------------------------------+-----------------------------------
#                                       0 |          1        0.63        0.63
# Reason does not apply to me apply at al |          2        1.27        1.90
#           Reason applies to me a little |          4        2.53        4.43
#                             Neither nor |         15        9.49       13.92
#           Reason somewhat applies to me |         76       48.10       62.03
#              Reason applies to me a lot |         60       37.97      100.00
# ----------------------------------------+-----------------------------------
#                                   Total |        158      100.00
# 
# -> tabulation of v_93  
# 
#            I was very interested in the |
#      replication aspect of the project. |      Freq.     Percent        Cum.
# ----------------------------------------+-----------------------------------
#                                       0 |          1        0.64        0.64
# Reason does not apply to me apply at al |          2        1.27        1.91
#           Reason applies to me a little |          3        1.91        3.82
#                             Neither nor |          4        2.55        6.37
#           Reason somewhat applies to me |         61       38.85       45.22
#              Reason applies to me a lot |         86       54.78      100.00
# ----------------------------------------+-----------------------------------
#                                   Total |        157      100.00
# 
# -> tabulation of v_94  
# 
# I expected to learn and to develop as a |
#                             researcher. |      Freq.     Percent        Cum.
# ----------------------------------------+-----------------------------------
#                                       0 |          2        1.27        1.27
# Reason does not apply to me apply at al |          2        1.27        2.55
#           Reason applies to me a little |         14        8.92       11.46
#                             Neither nor |         19       12.10       23.57
#           Reason somewhat applies to me |         74       47.13       70.70
#              Reason applies to me a lot |         46       29.30      100.00
# ----------------------------------------+-----------------------------------
#                                   Total |        157      100.00
# 
# -> tabulation of v_95  
# 
# The CRI seemed like a valuable addition |
#                               to my CV. |      Freq.     Percent        Cum.
# ----------------------------------------+-----------------------------------
# Reason does not apply to me apply at al |         31       19.75       19.75
#           Reason applies to me a little |         43       27.39       47.13
#                             Neither nor |         43       27.39       74.52
#           Reason somewhat applies to me |         32       20.38       94.90
#              Reason applies to me a lot |          8        5.10      100.00
# ----------------------------------------+-----------------------------------
#                                   Total |        157      100.00
# 
# -> tabulation of v_96  
# 
#  I joined because I know one or more of |
#                         the organizers. |      Freq.     Percent        Cum.
# ----------------------------------------+-----------------------------------
# Reason does not apply to me apply at al |        108       68.79       68.79
#           Reason applies to me a little |          9        5.73       74.52
#                             Neither nor |         13        8.28       82.80
#           Reason somewhat applies to me |         19       12.10       94.90
#              Reason applies to me a lot |          8        5.10      100.00
# ----------------------------------------+-----------------------------------
#                                   Total |        157      100.00
# 
# -> tabulation of v_98  
# 
#              Not enough time |      Freq.     Percent        Cum.
# -----------------------------+-----------------------------------
#                            0 |          1        0.64        0.64
#         Did not constrain me |          8        5.10        5.73
# Constrained me only a little |         26       16.56       22.29
#      Constrained me somewhat |         51       32.48       54.78
#  Constrained me considerably |         71       45.22      100.00
# -----------------------------+-----------------------------------
#                        Total |        157      100.00
# 
# -> tabulation of v_99  
# 
#  Inadequate materials (e.g., |
# software or computing power) |      Freq.     Percent        Cum.
# -----------------------------+-----------------------------------
#         Did not constrain me |        110       70.06       70.06
# Constrained me only a little |         27       17.20       87.26
#      Constrained me somewhat |         13        8.28       95.54
#  Constrained me considerably |          7        4.46      100.00
# -----------------------------+-----------------------------------
#                        Total |        157      100.00
# 
# -> tabulation of v_100  
# 
#    Not enough methods skills |      Freq.     Percent        Cum.
# -----------------------------+-----------------------------------
#         Did not constrain me |         65       41.40       41.40
# Constrained me only a little |         53       33.76       75.16
#      Constrained me somewhat |         30       19.11       94.27
#  Constrained me considerably |          9        5.73      100.00
# -----------------------------+-----------------------------------
#                        Total |        157      100.00
# 
# -> tabulation of v_101  
# 
#          Not enough software |
#           programming skills |      Freq.     Percent        Cum.
# -----------------------------+-----------------------------------
#         Did not constrain me |        110       70.06       70.06
# Constrained me only a little |         35       22.29       92.36
#      Constrained me somewhat |         12        7.64      100.00
# -----------------------------+-----------------------------------
#                        Total |        157      100.00
# 
# -> tabulation of v_102  
# 
#  Having strict deadlines was |
#             a problem for me |      Freq.     Percent        Cum.
# -----------------------------+-----------------------------------
#         Did not constrain me |         40       25.48       25.48
# Constrained me only a little |         49       31.21       56.69
#      Constrained me somewhat |         48       30.57       87.26
#  Constrained me considerably |         20       12.74      100.00
# -----------------------------+-----------------------------------
#                        Total |        157      100.00
```

### Individual Measurement Method

```{r}

# Stata data has labels, have to get original codes instead

l1 <- get.label(cri_part, "v_98")
l2 <- get.label(cri_part, "v_34")
l3 <- get.label(cri_part, "v_17")
l4 <- get.label(cri_part, "v_31")
l5 <- get.label(cri_part, "v_88")
l6 <- get.label(cri_part, "v_35")

# Belief that H1 is true
cri_part$belief1 <- get.origin.codes(cri_part$belief_H1_1, l4)
cri_part$belief2 <- get.origin.codes(cri_part$belief_agecare_1, l4)
cri_part$belief3 <- get.origin.codes(cri_part$belief_unempl_1, l4)
cri_part$belief4 <- get.origin.codes(cri_part$belief_income_1, l4)
cri_part$belief5 <- as.numeric(cri_part$belief_housing_1)
cri_part$belief6 <- get.origin.codes(cri_part$belief_labour_1, l4)
cri_part$belief7 <- get.origin.codes(cri_part$belief_health_1, l4)

# Topical knowledge/experience

cri_part$topic1 <- as.numeric(cri_part$v_17)
cri_part$topic2 <- as.numeric(cri_part$v_19)
cri_part$topic3 <- as.numeric(cri_part$v_20)
cri_part$topic4 <- get.origin.codes(cri_part$v_88, l5)
cri_part$topic5 <- get.origin.codes(cri_part$v_35, l6)
cri_part$topic5 <- 1+(-1*cri_part$topic5)
cri_part$topic6 <- get.origin.codes(cri_part$v_37, l6)
cri_part$topic7 <- get.origin.codes(cri_part$v_38, l6)
cri_part$topic8 <- get.origin.codes(cri_part$v_39, l6)
cri_part$topic9 <- get.origin.codes(cri_part$v_40, l6)

# Statistics skills/experience

cri_part$stat1 <- get.origin.codes(cri_part$v_100, l1) # higher values = restricted by lack of methods skills, reverse the coding
cri_part$stat1 <- (cri_part$stat1*-1)+5
cri_part$stat2 <- get.origin.codes(cri_part$v_101, l1) # higher values = restricted by lack of methods skills, reverse the coding
cri_part$stat2 <- (cri_part$stat2*-1)+5
cri_part$stat3 <- get.origin.codes(cri_part$v_34, l2)

cri_part$stat4 <- as.numeric(ifelse(cri_part$backgr_exp_teach_stat == "<NA>", NA, ifelse(cri_part$backgr_exp_teach_stat == "10+", 12, cri_part$backgr_exp_teach_stat)))

cri_part$stat5 <- as.numeric(ifelse(cri_part$backgr_exp_famil_mlm == "<NA>", NA, ifelse(cri_part$backgr_exp_famil_mlm == "_(1)", 1, ifelse(cri_part$backgr_exp_famil_mlm == "_(2)", 2, ifelse(cri_part$backgr_exp_famil_mlm == "_(3)", 3, ifelse(cri_part$backgr_exp_famil_mlm == "_(4)", 4, 5))))))

cri_part$stat6 <- as.numeric(cri_part$v_18)
cri_part$stat7 <- as.numeric(cri_part$v_21)

# question is wether immigration laws should be made tougher (higher scores) versus laxer (lower), need to reverse code
cri_part$pro_immigrant <- as.numeric(cri_part$attitude_immigration_1)
cri_part$pro_immigrant <- (cri_part$pro_immigrant*-1)+7

cri_part <- select(cri_part, u_teamid, belief1:pro_immigrant)
# cri_indiv.csv
             
```

### Team Measurement Method

Note we take the MAX for topic & statistics, MEAN for belief and pro-immigrant


```{r recodes, warning=F, message=F}

#Generate team-specific qualities

# Because the CRI required teams to define the data-generating model (or at least a test of it) and use statistical competencies to carry it out, we suspect that the highest statistical and topical knowledge and experience per team will have the greatest influence on the results. Therefore, for the concepts 'Stats Level' and 'Topic Level' we take the row min or max for each team (depending on coding. 

cri <- cri %>%
  rowwise() %>%
      mutate(stat1 = 4+(-1*min(v_1001, v_1002, v_1003, na.rm = T)), #lower is less constrained
             stat2 = 4+(-1*min(v_1011, v_1012, v_1013, na.rm = T)), #lower is less constrained
             stat3 = max(v_341, v_342, v_343, na.rm = T), #higher is less dificult
             stat4 = max(backgr_exp_teach_stat1, backgr_exp_teach_stat2, backgr_exp_teach_stat3, na.rm = T), #higher more courses taught
             stat5 = max(backgr_exp_famil_mlm1, backgr_exp_famil_mlm2, backgr_exp_famil_mlm3, na.rm = T), #higher more MLM familiarity
             stat6 = max(v_181, v_182, v_183, na.rm = T), #higher is more stat publications
             stat7 = max(v_211, v_212, v_213, na.rm = T), #higher is more MLM publications
             topic1 = max(v_171, v_172, v_173, na.rm = T), #higher is more Imm pubs
             topic2 = max(v_191, v_192, v_193, na.rm = T), #higher is more Policy pubs
             topic3 = max(v_201, v_202, v_203, na.rm = T), #higher is more Policy/Opinion pubs
             topic4 = max(v_881, v_882, v_883, na.rm = T), #higher is more interest in topic
             topic5 = 1+(-1*min(v_351, v_352, v_353, na.rm = T)), #lower not relevant
             topic6 = max(v_371, v_372, v_373, na.rm = T), #Higher relevant
             topic7 = max(v_381, v_382, v_383, na.rm = T), #Higher relevant
             topic8 = max(v_391, v_392, v_393, na.rm = T), #Higher relevant
             topic9 = max(v_401, v_402, v_403, na.rm = T), #Higher relevant
             belief1 = mean(c(belief_H1_11,belief_H1_12,belief_H1_13), na.rm = T), # higher more belief
             belief2 = mean(c(belief_agecare_11,belief_agecare_12,belief_agecare_13), na.rm = T), # higher more belief
             belief3 = mean(c(belief_unempl_11,belief_unempl_12,belief_unempl_13), na.rm = T), # higher more belief
             belief4 = mean(c(belief_income_11,belief_income_12,belief_income_13), na.rm = T), # higher more belief
             belief5 = mean(c(belief_housing_11,belief_housing_12,belief_housing_13), na.rm = T), # higher more belief
             belief6 = mean(c(belief_labour_11,belief_labour_12,belief_labour_13), na.rm = T), # higher more belief
             belief7 = mean(c(belief_health_11,belief_health_12,belief_health_13), na.rm = T), # higher more belief
             belief_strength = max(belief_certainty_11,belief_certainty_12,belief_certainty_13, na.rm = T),
             pro_immigrant = mean(c(attitude_immigration_11,attitude_immigration_12,attitude_immigration_13), na.rm = T),
             pro_immigrant = (pro_immigrant*-1)+7, # question is wether immigration laws should be made tougher (higher scores) versus laxer (lower), need to reverse code
             changemind_delib = max(delib_changemind1,delib_changemind2,delib_changemind3, na.rm = T),
             topic_interest = max(v_881,v_882,v_883, na.rm=T),
             pub_interest = max(v_901,v_902,v_903, na.rm=T))


cri <- cri %>%
         mutate(belief_strength = ifelse(belief_strength == "Inf", NA, belief_strength),
                belief_strength = ifelse(belief_strength == "-Inf", NA, belief_strength),
                pro_immigrant = ifelse(pro_immigrant == "Inf", NA, pro_immigrant),
                pro_immigrant = ifelse(pro_immigrant == "-Inf", NA, pro_immigrant),
                topic_interest = ifelse(topic_interest == "Inf", NA, topic_interest),
                topic_interest = ifelse(topic_interest == "-Inf", NA, topic_interest),
                pub_interest = ifelse(pub_interest == "Inf", NA, pub_interest),
                pub_interest = ifelse(pub_interest == "-Inf", NA, pub_interest),
                changemind_delib = ifelse(changemind_delib == "Inf", NA, changemind_delib),
                changemind_delib = ifelse(changemind_delib == "-Inf", NA, changemind_delib))

```


### Merge in team size variable

```{r teamsize}

teamsize <- read.csv(file = "data/teamsize.csv", header = T)

cri <- left_join(cri, teamsize, by = "u_teamid")

rm(teamsize, cri2)

```

### Make Scales of Team Qualities

### Mesaurement of Participant Team Qualities

#### Correlations of Participant Survey Responses

Aggregated by team

```{r f3}
# make a dataset for correlation

cor <- select(cri, stat1,stat2,stat3,stat4,stat5,stat6,stat7,topic1,topic2,topic3,topic4,topic5,topic6,topic7,topic8,topic9,belief1,belief2,belief3,belief4,belief5,belief6,belief7,belief_strength,pro_immigrant,u_teamid)

cor <- aggregate(cor, by=list(cor$u_teamid), FUN=mean, na.rm=T)
cor <- replace(cor, cor=="-Inf",NA)

# team 27 missing 3 values, fill them in for consistency, fill represents a teaching of 12 stats courses score
cor <- cor %>%
  mutate(stat1 = ifelse(u_teamid == 27,3,stat1),
         stat2 = ifelse(u_teamid == 27,3,stat2),
         topic4 = ifelse(u_teamid == 27,4,topic4))

# drop Brady and Finnigan's original analysis
cor <- completeFun(cor, "stat1")

cor1 <- cor(cor, use = "pairwise.complete.obs")

kable_styling(kable(cor1, digits = 2))


```

#### Measurement Model

```{r SEM}

pacman::p_load("lavaan")

cfa <- ' stat =~ stat1 + stat2 + stat3 + stat4 + stat5 + stat6 + stat7
         topic =~ topic1 + topic2 + topic3 + topic4 + topic5 + topic6 + topic7 + topic8 + topic9
         belief =~ belief1 + belief2 + belief3 + belief4 + belief5 + belief6 + belief7 '
fit <- cfa(cfa, data = cor)
summary(fit, fit.measures=TRUE)

# predicted factor scores
cor1 <- as.data.frame(lavPredict(fit))
cor <- cbind(cor,cor1)

# put them back into the main data
cor1 <- select(cor, u_teamid, stat, topic, belief)

cri <- left_join(cri,cor1,by = "u_teamid")




```

### Merge in Subjective Voting

The file popdf_out.Rdata is needed for this, see CRI_Subj_Votes.R add link

```{r votes, message = F, warning=F}

load(file = "data/popdf_out.Rdata")

cri <- left_join(cri, popdf_out, by = "id")

# create levels of subjective responses and voting
criT <- cri %>%
  mutate(stat_cat = ifelse(stat > 0.45, "Highest", ifelse(stat > 0, "High", ifelse(stat > -0.3, "Low", "Lowest"))),
         belief_cat = ifelse(belief > 0.4, "Highest", ifelse(belief > 0.10, "High", ifelse(belief > -0.39, "Low", "Lowest"))),
         topic_cat = ifelse(topic > 0.4, "Highest", ifelse(topic > 0, "High", ifelse(topic > -0.4, "Low", "Lowest"))),
         total_score_cat = ifelse(total_score > 0.53, "Highest", ifelse(total_score > 0.41, "High", ifelse(total_score > 0.32, "Low", "Lowest"))))

criT <- select(criT, stat_cat, belief_cat, topic_cat, total_score_cat, id)

cri_str <- left_join(cri_str, criT, by = "id")

# cri <- select(cri, -c(u_expgroup1:belief7))

rm(cor, cor1, fit, popdf_out, criT)

```

### Number of positive test results per team

```{r postest}
cri <- cri %>%
  group_by(u_teamid,Hresult) %>%
  mutate(pos_test_pct_p05 = sum(AME_sup_p05, na.rm = T)/inv_weight,
         pos_test_pct_p10 = sum(AME_sup_p10, na.rm = T)/inv_weight) %>%
  ungroup()
```


#### Team Level Datasets

We create an overall team average dataset, a flow models only team dataset and a stock models only team dataset.

```{r team_data, message = F, warning=F}

cri_team <- cri
cri_team_stock <- subset(cri, cri$Stock == 1)
cri_team_flow <- subset(cri, cri$Flow == 1)

cri_team <- aggregate(cri_team, by = list(cri_team$u_teamid, cri_team$Hresult), FUN = mean, na.rm = T)
cri_team_stock <- aggregate(cri_team_stock, by = list(cri_team_stock$u_teamid, cri_team_stock$Hresult), FUN = mean, na.rm = T)
cri_team_flow <- aggregate(cri_team_flow, by = list(cri_team_flow$u_teamid, cri_team_flow$Hresult), FUN = mean, na.rm = T)

# can't aggregate strings so we have to run recodes again

cri_team <- cri_team %>%
  mutate(stat_cat = ifelse(stat > 0.75, "Highest", ifelse(stat > .25, "High", ifelse(stat > -0.25, "Low", "Lowest"))),
         belief_cat = ifelse(belief > 0.6, "Highest", ifelse(belief > 0.25, "High", ifelse(belief > -0.25, "Low", "Lowest"))),
         topic_cat = ifelse(topic > 0.75, "Highest", ifelse(topic > 0.25, "High", ifelse(topic > -0.25, "Low", "Lowest"))),
         total_score_cat = ifelse(total_score > 0.53, "Highest", ifelse(total_score > 0.41, "High", ifelse(total_score > 0.34, "Low", "Lowest"))),
         Hresult = ifelse(Hnotest==1, "No test", ifelse(Hsupport==1, "Support", ifelse(Hreject==1, "Reject", ifelse((Hnotest_stock==1 & Stock==1) | (Hnotest_net==1 & Flow==1), "No test", ifelse((Hsupport_stock==1 & Stock ==1) | (Hsupport_net==1 & Flow==1), "Support", ifelse((Hreject_stock==1 & Stock==1) | (Hreject_net==1 & Flow==1), "Reject", NA)))))))

# repeat for stock
cri_team_stock <- cri_team_stock %>%
  mutate(stat_cat = ifelse(stat > 0.75, "Highest", ifelse(stat > .25, "High", ifelse(stat > -0.25, "Low", "Lowest"))),
         belief_cat = ifelse(belief > 0.6, "Highest", ifelse(belief > 0.25, "High", ifelse(belief > -0.25, "Low", "Lowest"))),
         topic_cat = ifelse(topic > 0.75, "Highest", ifelse(topic > 0.25, "High", ifelse(topic > -0.25, "Low", "Lowest"))),
         total_score_cat = ifelse(total_score > 0.53, "Highest", ifelse(total_score > 0.41, "High", ifelse(total_score > 0.34, "Low", "Lowest"))),
         Hresult = ifelse(Hnotest==1, "No test", ifelse(Hsupport==1, "Support", ifelse(Hreject==1, "Reject", ifelse((Hnotest_stock==1 & Stock==1) | (Hnotest_net==1 & Flow==1), "No test", ifelse((Hsupport_stock==1 & Stock ==1) | (Hsupport_net==1 & Flow==1), "Support", ifelse((Hreject_stock==1 & Stock==1) | (Hreject_net==1 & Flow==1), "Reject", NA)))))))

#repeat for flow
cri_team_flow <- cri_team_flow %>%
  mutate(stat_cat = ifelse(stat > 0.75, "Highest", ifelse(stat > .25, "High", ifelse(stat > -0.25, "Low", "Lowest"))),
         belief_cat = ifelse(belief > 0.6, "Highest", ifelse(belief > 0.25, "High", ifelse(belief > -0.25, "Low", "Lowest"))),
         topic_cat = ifelse(topic > 0.75, "Highest", ifelse(topic > 0.25, "High", ifelse(topic > -0.25, "Low", "Lowest"))),
         total_score_cat = ifelse(total_score > 0.53, "Highest", ifelse(total_score > 0.41, "High", ifelse(total_score > 0.34, "Low", "Lowest"))),
         Hresult = ifelse(Hnotest==1, "No test", ifelse(Hsupport==1, "Support", ifelse(Hreject==1, "Reject", ifelse((Hnotest_stock==1 & Stock==1) | (Hnotest_net==1 & Flow==1), "No test", ifelse((Hsupport_stock==1 & Stock ==1) | (Hsupport_net==1 & Flow==1), "Support", ifelse((Hreject_stock==1 & Stock==1) | (Hreject_net==1 & Flow==1), "Reject", NA)))))))
# remove columns that contain all NAs
 
cri_team <- Filter(function(x) !all(is.na(x)), cri_team)
cri_team_stock <- Filter(function(x) !all(is.na(x)), cri_team_stock)
cri_team_flow <- Filter(function(x) !all(is.na(x)), cri_team_flow)

# add missing variable
cri_stra <- select(cri_str, dv_type, id)
df <- left_join(cri, cri_stra, by = "id")
rm(cri_stra)
```




Save Point



```{r save_out, warning=F,message=F}

# save as csv to ensure no attributes carry over
write.csv(cri, file = "data/cri.csv", row.names = F)
write.csv(cri_str, file = "data/cri_str.csv", row.names = F)
write.csv(cri_team, file = "data/cri_team.csv", row.names = F)
write.csv(cri_part, file = "data/cri_indv.csv", row.names = F)

# merge flow and stock team-level results for plotting

cri_team_combine <- bind_rows(cri_team_stock, cri_team_flow)

# remove missing
cri_team_combine <- subset(cri_team_combine, !is.na(AME))
cri_team_combine <- subset(cri_team_combine, !is.na(stat_cat))

write.csv(cri_team, file = "data/cri_team_combine.csv", row.names = F)


```


