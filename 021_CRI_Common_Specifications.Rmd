---
title: "021_CRI_Unique_Cases"
author: "Nate Breznau"
date: "12/18/2020"
output: html_document
---





### Setup


```{r setup, include=FALSE}
rm(list = ls())
library(pacman)


pacman::p_load("devtools","ggplot2","tidyverse","readr","ExPanDaR","plotscale","lattice","tidyr","mlogit","knitr","grid","zoo","ggpubr","ragg","factoextra","cluster","kableExtra","skimr","WeightedCluster")

# Load Researcher Degrees of Freedom Analysis package
#  
# devtools::install_github("joachim-gassen/rdfanalysis")
library(rdfanalysis)


```


```{r load, warning=F,message=F, include=F}

cri <- read.csv(file = "data/cri.csv", header = T)
cri_str <- read.csv(file = "data/cri_str.csv", header = T)
cri_team <- read.csv(file = "data/cri_team.csv", header = T)

```


## Dissimilarity Analysis

### Model Specification Count

The problem is that many of these model specifications were dropped because they were not the team's preferred models, or because in the margins extraction phase the PIs decided that the models were not preferable (only when the team provided them as 'extra' models). Therefore, some columns of the database have zero variance. These columns need to be removed.

```{r qual_codes, warning = F}
# Get data with only model specs
cri_spec_only <- cri

         
cri_spec_only <- select(cri_spec_only, -c(count:main_IV_measurement, num_countries, inv_weight, additionalinfo, Hsupport:HresultF, AME_sup_p05:MODEL_SCORE))



# convert character to numeric
cri_spec_only <- cri_spec_only %>%
  mutate(main_IV_time = as.numeric(as.factor(main_IV_time)),
         main_IV_effect = as.numeric(as.factor(main_IV_effect)),
         package = as.numeric(as.factor(package)))

# specifications with at least three teams using them
rownames(cri_spec_only) <- cri_spec_only$id
cri_spec_only_team <- aggregate(cri_spec_only, by = list(cri_spec_only$u_teamid), FUN = "mean")
# our analysis below revealed we should try a subset
cri_spec_only2 <- subset(cri_spec_only, u_teamid != 10 & u_teamid != 61 & u_teamid != 82)
cri_spec_only <- select(cri_spec_only,-c(u_teamid, id))
cri_spec_only2 <- select(cri_spec_only2,-c(u_teamid, id))
cri_sums <- colSums(Filter(is.numeric, cri_spec_only_team))
cri_sums <- as.data.frame(c("sum",cri_sums))
cri_sums[,1] <- as.numeric(cri_sums[,1])
cri_sums_uncommon <- subset(cri_sums, cri_sums[,1] < 3)

# remove zero variance columns
cri_spec_only <- cri_spec_only[ - as.numeric(which(apply(cri_spec_only, 2, var) == 0))]
cri_spec_only2 <- cri_spec_only2[ - as.numeric(which(apply(cri_spec_only2, 2, var) == 0))]

```


We identified `r nrow(cri_sums) - 2` model specifications in our qualitative coding of the team's submitted code. Of these, `r ((nrow(cri_sums) - 2) - length(cri_sums_uncommon[,1]))` were *common* across teams (occurred in three or more) and `r length(cri_sums_uncommon[,1])` were *idiosyncratic* specifications (occurred in two or less teams). Of these model specifications, `r nrow(cri_spec_only)` have variance (the remainder refer to specifications in models that were dropped before the final preferred models; usually those that were seen as sensitivity models rather than main models, or those that the team in the end decided against through correspondence and error checking - remember teams could change their models at any point if they desired or had new information although this was rare).





### Dissimilarity in Decision Realms

However, a second look at these data suggests that we may want to separately cluster on different decisions.
1) Countries and waves
2) Variables
3) Estimation, measurement, model structure
4) All at once

```{r diss_1}
# select only country and wave variables
cri_spec_only01 <- select(cri_spec_only, w1985:TR)
# variables in the model
cri_spec_only02 <- select(cri_spec_only, main_IV_as_control, emigration_ivC:fbXleftright)
# est & measurement (we provide two versions, one with DVs, one without)
cri_spec_only03 <- select(cri_spec_only, main_IV_time, main_IV_effect, Jobs:ChangeFlow, twowayfe:pseudo_pnl, dichotomize)
cri_spec_only03 <- cri_spec_only03 %>%
  mutate(main_IV_time_1year = car::recode(main_IV_time, "1 = 1; c(2,3,4,5,6) = 0"),
         main_IV_time_10year = car::recode(main_IV_time, "2 = 1; c(1,3,4,5,6) = 0"),
         main_IV_time_3year = car::recode(main_IV_time, "3 = 1; c(1,2,4,5,6)= 0"),
         main_IV_time_5year = car::recode(main_IV_time, "4 = 1; c(1,2,3,5,6)= 0"),
         main_IV_time_current = car::recode(main_IV_time, "5 = 1; c(1,2,3,4,6) = 0"),
         main_IV_time_perwave = car::recode(main_IV_time, "6 = 1; c(1,2,3,4,5) = 0"),
         main_IV_effect_between = car::recode(main_IV_effect, "1 = 1; c(2,3,4) =0"),
         main_IV_effect_total = car::recode(main_IV_effect, "2 = 1; c(1,3,4) = 0"),
         main_IV_effect_unclear = car::recode(main_IV_effect, "3 = 1; c(1,2,4) = 0"),
         main_IV_effect_within = car::recode(main_IV_effect, "4 = 1; c(1,2,3) = 0"))
cri_spec_only03 <- select(cri_spec_only03, -c(main_IV_time, main_IV_effect))
cri_spec_only03_noDV <- select(cri_spec_only03, -c(Jobs:Health))
#make a test for all three at once
cri_spec_only04 <- select(cri_spec_only, w1985:TR, main_IV_as_control, emigration_ivC:fbXleftright)
cri_spec_only04 <- cbind(cri_spec_only04, cri_spec_only03)
cri_spec_only04_noDV <- cbind(cri_spec_only04, cri_spec_only03_noDV)
# find and merge identical cases

cri$identical_samples <- wcAggregateCases(cri_spec_only01)[["disaggIndex"]]
cri$identical_variables <- wcAggregateCases(cri_spec_only02)[["disaggIndex"]]
cri$identical_estmodel <- wcAggregateCases(cri_spec_only03)[["disaggIndex"]]
cri$identical_estmodel_noDV <- wcAggregateCases(cri_spec_only03_noDV)[["disaggIndex"]]
cri$identical_all <- wcAggregateCases(cri_spec_only04)[["disaggIndex"]]
cri$identical_all_noDV <- wcAggregateCases(cri_spec_only04_noDV)[["disaggIndex"]]

# need a function to mark all duplicates, not just the subsequent instances
allDuplicated <- function(vec){
  front <- duplicated(vec)
  back <- duplicated(vec, fromLast = TRUE)
  all_dup <- front + back > 0
  return(all_dup)
}
# absolutely unique cases coded to zero
cri <- cri %>%
  mutate(identical_samples = ifelse(allDuplicated(identical_samples) == FALSE, 0, identical_samples),
         identical_variables = ifelse(allDuplicated(identical_variables) == FALSE, 0, identical_variables),
         identical_estmodel = ifelse(allDuplicated(identical_estmodel) == FALSE, 0, identical_estmodel),
         identical_estmodel_noDV = ifelse(allDuplicated(identical_estmodel_noDV) == FALSE, 0, identical_estmodel_noDV),
         identical_all = ifelse(allDuplicated(identical_all) == FALSE, 0, identical_all),
         identical_all_noDV = ifelse(allDuplicated(identical_all_noDV) == FALSE, 0, identical_all))
```

In our coding of model specifications, `r length(cri$identical_all[cri$identical_all == 0])` of `r length(cri$identical_all)` models are unique; and of the remaining `r length(cri$identical_all) - length(cri$identical_all[cri$identical_all == 0])` models, all are identical with only one other model and each pair was created by the same team. So across teams there are no identical models when considering our identified `r nrow(cri_sums) - 2` model specifications. However, we instructed the teams to use six questions from the survey data as their dependent variables. Most teams chose to analyze these variables separately, therefore when we exclude these six variables from the model specifications there are only `r length(cri$identical_all_noDV[cri$identical_all_noDV == 0])` unique models, but all repeated models were produced within individual teams; i.e., no identical models across teams were identified.

We investigated model specification as three different domains: sample selection (which countries and waves), independent variables included in the model and finally estimation, measurement strategy and equation format. 

```{r desc_samples}
cri_samples <- cri %>%
    # select(AME_Z, Hsup, identical_samples) %>%
    group_by(identical_samples) %>%
    summarize(AME_Zm = round(mean(AME_Z, na.rm = T),3),
              AME_Zsd = round(sd(AME_Z, na.rm = T),3),
              cases = n(),
              teams = length(unique(u_teamid)),
              Hsupm = round(mean(Hsup, na.rm = T),3),
              Hsupsd = round(sd(Hsup, na.rm = T),3)) 
cri_identical <- cri_identical[order(cri_identical$cases, decreasing = T),]
head(cri_identical)
```
We observed three somewhat common sample selections that accounted for 245, 102 and 96 models respectively that were used across 11, 8 and 10 teams respectively. However, when looking at the average of the resulting effects we see no pattern (in none of the three identical sample selection groups was the average significantly different from zero). 




```{r desc_variables}
cri_variables <- cri %>%
    # select(AME_Z, Hsup, identical_samples) %>%
    group_by(identical_variables) %>%
    summarize(AME_Zm = round(mean(AME_Z, na.rm = T),3),
              AME_Zsd = round(sd(AME_Z, na.rm = T),3),
              cases = n(),
              teams = length(unique(u_teamid)),
              Hsupm = round(mean(Hsup, na.rm = T),3),
              Hsupsd = round(sd(Hsup, na.rm = T),3)) 
cri_variables <- cri_variables[order(cri_variables$cases, decreasing = T),]
head(cri_variables)
```

```{r desc_estmodel}
cri_estmodel <- cri %>%
    # select(AME_Z, Hsup, identical_samples) %>%
    group_by(identical_estmodel) %>%
    summarize(AME_Zm = round(mean(AME_Z, na.rm = T),3),
              AME_Zsd = round(sd(AME_Z, na.rm = T),3),
              cases = n(),
              teams = length(unique(u_teamid)),
              Hsupm = round(mean(Hsup, na.rm = T),3),
              Hsupsd = round(sd(Hsup, na.rm = T),3)) 
cri_estmodel <- cri_estmodel[order(cri_estmodel$cases, decreasing = T),]
head(cri_estmodel)
```