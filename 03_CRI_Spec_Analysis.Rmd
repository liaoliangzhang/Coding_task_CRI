---
title: "Tech 3, Specification Curves"
output:
  html_document:
    df_print: paged
    always_allow_html: true
  html_notebook: default
---



### Setup

We load the rdfanalysis package designed by <a href=https://joachim-gassen.github.io/rdfanalysis/>Joachim Gassen</a> to generate specification curves.
   
   

```{r setup, include=FALSE}
rm(list = ls())
library(pacman)


pacman::p_load("devtools","ggplot2","tidyverse","readr","ExPanDaR","plotscale","lattice","tidyr","mlogit","knitr","grid","zoo","ggpubr","ragg","factoextra","cluster","kable","kableExtra")

# Load Researcher Degrees of Freedom Analysis package
#  
# devtools::install_github("joachim-gassen/rdfanalysis")
library(rdfanalysis)


```


```{r load, warning=F,message=F, include=F}

cri <- read.csv(file = "data/cri.csv", header = T)
cri_str <- read.csv(file = "data/cri_str.csv", header = T)
cri_team <- read.csv(file = "data/cri_team.csv", header = T)

```


## Dissimilarity Analysis

### Model Specification Count

The problem is that many of these model specifications were dropped because they were not the team's preferred models, or because in the margins extraction phase the PIs decided that the models were not preferable (only when the team provided them as 'extra' models). Therefore, some columns of the database have zero variance. These columns need to be removed.

```{r qual_codes, warning = F}
# Get data with only model specs
cri_spec_only <- cri

         
cri_spec_only <- select(cri_spec_only, -c(count:main_IV_measurement, num_countries, inv_weight, additionalinfo, Hsupport:HresultF, AME_sup_p05:MODEL_SCORE))



# convert character to numeric
cri_spec_only <- cri_spec_only %>%
  mutate(main_IV_time = as.numeric(as.factor(main_IV_time)),
         main_IV_effect = as.numeric(as.factor(main_IV_effect)),
         package = as.numeric(as.factor(package)))

# specifications with at least three teams using them
rownames(cri_spec_only) <- cri_spec_only$id
cri_spec_only_team <- aggregate(cri_spec_only, by = list(cri_spec_only$u_teamid), FUN = "mean")
# our analysis below revealed we should try a subset
cri_spec_only2 <- subset(cri_spec_only, u_teamid != 10 & u_teamid != 61 & u_teamid != 82)
cri_spec_only <- select(cri_spec_only,-c(u_teamid, id))
cri_spec_only2 <- select(cri_spec_only2,-c(u_teamid, id))
cri_sums <- colSums(Filter(is.numeric, cri_spec_only_team))
cri_sums <- as.data.frame(c("sum",cri_sums))
cri_sums[,1] <- as.numeric(cri_sums[,1])
cri_sums_uncommon <- subset(cri_sums, cri_sums[,1] < 3)

# remove zero variance columns
cri_spec_only <- cri_spec_only[ - as.numeric(which(apply(cri_spec_only, 2, var) == 0))]
cri_spec_only2 <- cri_spec_only2[ - as.numeric(which(apply(cri_spec_only2, 2, var) == 0))]

```


We identified `r nrow(cri_sums) - 2` model specifications in our qualitative coding of the team's submitted code. Of these, `r ((nrow(cri_sums) - 2) - length(cri_sums_uncommon[,1]))` were *common* across teams (occurred in three or more) and `r length(cri_sums_uncommon[,1])` were *idiosyncratic* specifications (occurred in two or less teams). Of these model specifications, `r nrow(cri_spec_only)` have variance (the remainder refer to specifications in models that were dropped before the final preferred models; usually those that were seen as sensitivity models rather than main models, or those that the team in the end decided against through correspondence and error checking - remember teams could change their models at any point if they desired or had new information although this was rare)



###

```{r diss}
# dissimilarity matrix
cri_dm <- dist(cri_spec_only, method = "euclidian")

# hierarchical clustering
cri_hc <- hclust(cri_dm, method = "ward.D2")

fviz_dend(cri_hc, rect = T)

# plot(cri_hc, cex = 0.5)

# k-means clustering
# cri_km <- eclust(cri_spec_only, "kmeans", nstart = 25)

```

This demonstrates 3 clusters, but teams 10, 61 and 82 are obvious outliers. Therefore, we re-run the analysis with them removed.

```{r clus_cut, warning = F, message = F, echo = F}
# k-means clustering
# cri_km2 <- eclust(cri_spec_only2, "kmeans", nstart = 25)

# saveRDS(cri_km2, file = "results/cri_km2.Rds")

readRDS(cri_km2, file = "results/cri_km2.Rds")

#recombine
cri_spec_only2$cluster <- cri_km2$cluster

cri_spec_only2$id <- rownames(cri_spec_only2)

#create merge file
cri_so2_merge <- select(cri_spec_only2, id, cluster)

cri <- left_join(cri, cri_so2_merge, by = "id")
cri$cluster <- ifelse(is.na(cri$cluster), 10, cri$cluster)

rm(cri_spec_only,cri_spec_only2,cri_spec_only_team,cri_sums_uncommon)
```




## Specification Curves

### Data Prep

```{r data_prep, warning=F, message=F, include=F}

# prep data
crispectest <- dplyr::select(cri_str, id, u_teamid, dv_type, iv_type, software, indepv, mator, dichotomize, twowayfe, cluster_any, Jobs:Scale,  w1985:w2016, p, AME_Z, lower_Z, upper_Z, id, eeurope, allavailable, mlm_any, Hresult, orig13, BELIEF_HYPOTHESIS, PRO_IMMIGRANT, TOPIC_KNOWLEDGE, STATISTICS_SKILL, MODEL_SCORE)

# create sample variables 
crispectest <- crispectest %>%
  mutate(w1985 = ifelse(is.na(w1985), "Other", w1985),
         w1990 = ifelse(is.na(w1990), "Other", w1990),
         w1996 = ifelse(is.na(w1996), "Other", w1996),
         w2006 = ifelse(is.na(w2006), "Other", w2006),
         w2016 = ifelse(is.na(w2016), "Other", w2016),
         orig13 = ifelse(is.na(orig13), "Other", orig13),
         eeurope = ifelse(is.na(eeurope), "Other", eeurope),
         allavailable = ifelse(is.na(allavailable), "Other", allavailable))

crispectest <- crispectest %>%
  mutate(SURVEY_WAVES = ifelse(w1985 == "Other" & w1990 == "Other" & w1996 == "w1996" & w2006 == "w2006" & w2016 == "Other", "1996 & 2006", ifelse(w1985 == "Other" & w1990 == "Other" & w1996 == "w1996" & w2006 == "w2006" & w2016 == "w2016", "1996, 2006 & 2016", ifelse(w1985 == "Other" & w1990 == "Other" & w1996 == "Other" & w2006 == "w2006" & w2016 == "w2016", "2006 & 2016", "Other"))),
         COUNTRIES = ifelse(orig13 == "orig13", "Core Rich Democracies Only", ifelse(allavailable == "allavailable", "All Available (~26)", ifelse(eeurope == "eeurope", "Includes Eastern/CE Europe", "Other"))))

#Remove NAs
crispectest <- subset(crispectest, !is.na(crispectest$AME_Z))
crispectest <- subset(crispectest, !is.na(crispectest$lower_Z))
  
  # some upper and lower bounds are exactly zero, adjust fix this
crispectest$lower_Z <- ifelse(crispectest$lower_Z > -0.00000001 & crispectest$lower_Z < 0.00000001, -0.0001, crispectest$lower_Z)
crispectest$upper_Z <- ifelse(crispectest$upper_Z > -0.00000001 & crispectest$upper_Z < 0.00000001, 0.0001, crispectest$upper_Z)

  #trim to have better plot range
  crispectest$lower_Z <- ifelse(crispectest$lower_Z < -0.75, -0.75, crispectest$lower_Z)
  crispectest$upper_Z <- ifelse(crispectest$upper_Z > 0.75, 0.75, crispectest$upper_Z)
  crispectest$upper_Z <- ifelse(crispectest$upper_Z < -0.75, -0.745, crispectest$upper_Z)
  crispectest$AME_Z <- ifelse(crispectest$AME_Z < -0.75, -0.747, crispectest$AME_Z)
  crispectest$AME_Z <- ifelse(crispectest$AME_Z > 0.75, 0.747, crispectest$AME_Z)
  
  #data prep
crispectest$est <- round(crispectest$AME_Z, 6)
crispectest$lb <- round(crispectest$lower_Z, 6)
crispectest$ub <- round(crispectest$upper_Z, 6)

# set up subjective conclusions at team level as 1 support, 0 not testable and -1 reject

cri_team <- cri_team %>%
  mutate(est = ifelse(Hsup == 1, 0.1, ifelse(Hrej == 1, -0.1, ifelse(Hno == 1, 0, NA))),
         ub = est+0.01,
         lb = est-0.01)

# import researcher aspects to crispectest
cri_merge <- select(cri, id, belief_ipred, pro_immigrant, topic_ipred, stats_ipred, total_score)

crispectest <- left_join(crispectest, cri_merge, by = "id")

```




### Average Marginal Effects

#### Spec Curve. Researcher Qualities and Subjective Model Scoring

```{r fig1_DV1, echo=F}



crispectest1 <- select(crispectest, BELIEF_HYPOTHESIS, PRO_IMMIGRANT, TOPIC_KNOWLEDGE, STATISTICS_SKILL, MODEL_SCORE, est, lb, ub)

crispectest1 <- subset(crispectest1, !is.na(crispectest1$PRO_IMMIGRANT))
crispectest1 <- subset(crispectest1, !is.na(crispectest1$TOPIC_KNOWLEDGE))
crispectest1 <- subset(crispectest1, !is.na(crispectest1$BELIEF_HYPOTHESIS))

  
# one problem with rdfanalysis is that it outputs the categories in the order they appear in the data, therefore we have to generate 3 rows of fake data to get the order right
  
fake <- as.data.frame(matrix(nrow = 3, ncol = 8))
fake[1,] <- c("Low","Low","Low","Low","Low",0,-0.00001,0.00001)
fake[2,] <- c("Mid","Mid","Mid","Mid","Mid",0,-0.00001,0.00001)
fake[3,] <- c("High","High","High","High","High",0,-0.00001,0.00001)
colnames(fake) <- c("BELIEF_HYPOTHESIS", "PRO_IMMIGRANT", "TOPIC_KNOWLEDGE", "STATISTICS_SKILL", "MODEL_SCORE", "est", "lb", "ub")
fake$est <- as.numeric(fake$est)
fake$ub <- as.numeric(fake$ub)
fake$lb <- as.numeric(fake$lb)

crispectest1 <- bind_rows(fake,crispectest1)

# remove/adjust attributes
  crispectest1 <- crispectest1[names(crispectest1)]
  choices <- 1:5
  attr(crispectest1, "choices") <- choices
  rownames(crispectest1) <- NULL
png(file = "results/Fig1_AME.png", width = 1200, height = 850, res = 144)

plot_rdf_spec_curve(crispectest1, "est", "lb", "ub", est_color = "grey", est_color_signeg = "red", est_color_sigpos = "blue", lower_to_upper = 1.5, est_label = "Average Marginal Effects\n(standardized)", choice_ind_point = F, pt_size = 0.3, ribbon = T)
dev.off()

knitr::include_graphics("results/Fig1_AME.png")
```

#### Fig 1. Alternative

Generate spec curves by hand, one ordered by effect size, the other by confidence.

```{r fig1_alt}
# remove original study
crispectest <- subset(crispectest, u_teamid != 0)

# order rows to make counter
crispectest <- crispectest[order(crispectest$est),]
crispectest$count <- 1:nrow(crispectest)

# create identifier of neg, ns, pos
crispectest <- crispectest %>%
  mutate(sig_group = as.factor(ifelse(est < 0 & ub > 0, NA, ifelse(est > 0 & lb < 0, NA, ifelse(est < 0 & ub < 0, 1, ifelse(est > 0 & lb > 0, 3, 2))))),
         est_sig = ifelse(!is.na(sig_group), est, NA),
         est_ns = ifelse(is.na(sig_group), est, NA),
         sig_group2 = as.factor(ifelse(is.na(sig_group), 2, sig_group)))

g1 <- ggplot(crispectest) +
  #geom_errorbar(aes(ymin = lb, ymax = ub), color = "thistle1") +
  geom_point(aes(x = count, y = est_ns), color = "grey55", shape = "|", size = 2, show.legend =F) + 
  geom_point(aes(x = count, y = est_sig, color = sig_group), shape = "|", size = 3) +
  scale_color_manual(values = c("chartreuse4","NA", "firebrick"), labels = c("Negative","Not sig.","Positive"," ")) +
  ylim(-0.21, 0.21) +
  labs(color = "Effect at 95%CI", x = "Model", y = " \n \nStandardized\nEffect Size") +
  theme_classic() +
  guides(color = guide_legend(override.aes = list(size=7, color=c("chartreuse4","grey55", "firebrick","NA")))) +
  theme(
    legend.key.size = unit(2,"line"),
    legend.position = "none")

```

```{r fig1_alt2}



# create identifier of neg, ns, pos
crispectest <- crispectest %>%
  mutate(ci = ifelse(sig_group2 == 1, ub, ifelse(sig_group2 == 3, lb, ifelse(est <= 0 & sig_group2 == 2, ub, ifelse(est > 0 & sig_group2 == 2, lb, NA)))),
         ub_order = ifelse(est <= 0 & sig_group2 == 1, 0, ifelse(est <= 0 & sig_group2==2, 1, ifelse(est > 0 & sig_group2==2, 2, ifelse(est > 0 & sig_group2 == 3, 3, NA)))),
         )

# order rows to make counter

crispectest <- crispectest[order(crispectest$ub_order, crispectest$ci),]
crispectest$count2 <- 1:nrow(crispectest)

# make an estimate where error bars show only for significant at 95% results
crispectest <- crispectest %>%
  mutate(est2 = ifelse(sig_group2 == 2, 0, est))

g2 <- ggplot(crispectest) +
  geom_errorbar(aes(x = count2, ymin = lb, ymax = ub), color = "grey55") +
  geom_point(aes(x = count2, y = est, color = sig_group2), size = 1) +
  scale_color_manual(values = c("chartreuse4","grey45", "firebrick"), labels = c("Negative","Not sig.","Positive")) +
  coord_cartesian(ylim = c(-0.21,0.21)) +
  labs(color = "Effect\nat 95%CI", x = "Model", y = "Effect Range", size = 2) +
  annotate("text", x = 150, y = 0.05,label = "Point Est. < 0\nOrdered by\nUpper CI", size = 2.5, color = "chartreuse4") +
  annotate("text", x = 1200, y = -0.05,label = "Point Est. > 0\nOrdered by\nLower CI", size = 2.5, color = "firebrick") +
  geom_vline(xintercept = 688) +
  theme_classic() +
  theme(
    legend.text = element_text(size = 6),
    legend.title = element_text(size = 8)
  )
  

```
#### Fig 1. Heatmap Concept

```{r rolling_average}
# create rolling average for effect ordering
crispectest <- crispectest[order(crispectest$count),]

# mean replacement, then rolling average, replace ends of rolling average w/ reg scores
crispectest <- crispectest %>%
  mutate(belief_ipredm = scale(ifelse(is.na(belief_ipred), mean(belief_ipred, na.rm=T), belief_ipred)),
         pro_immigrantm = scale(ifelse(is.na(pro_immigrant), mean(pro_immigrant, na.rm=T), pro_immigrant)),
         topic_ipredm = scale(ifelse(is.na(topic_ipred), mean(topic_ipred, na.rm=T), topic_ipred)),
         stats_ipredm = scale(ifelse(is.na(stats_ipred), mean(stats_ipred, na.rm=T), stats_ipred)),
         total_scorem = scale(ifelse(is.na(total_score), mean(total_score, na.rm=T), total_score)),
         belief_roll = zoo::rollmean(belief_ipredm, k = 30, fill = NA),
         pro_roll = zoo::rollmean(pro_immigrantm, k = 30, fill = NA),
         topic_roll = zoo::rollmean(topic_ipredm, k = 30, fill = NA),
         stats_roll = zoo::rollmean(stats_ipredm, k = 30, fill = NA),
         total_roll = zoo::rollmean(total_scorem, k = 30, fill = NA),
         belief_roll = ifelse(is.na(belief_roll), mean(belief_ipredm[count>1230,]), ifelse(is.na(belief_roll) & count < 30, mean(belief_ipredm[count<40,]), belief_roll)),
         pro_roll = ifelse(is.na(pro_roll) & count > 1230, mean(pro_immigrantm[count>1230,]), ifelse(is.na(pro_roll) & count < 30, mean(pro_immigrantm[count < 40,]), pro_roll)),
         topic_roll = ifelse(is.na(topic_roll) & count > 1230, mean(topic_ipredm[count>1230,]), ifelse(is.na(topic_roll) & count < 30, mean(topic_ipredm[count<30,]), topic_roll)),
         stats_roll = ifelse(is.na(stats_roll) & count > 1230, mean(stats_ipredm[count>1215,]), ifelse(is.na(stats_roll) & count < 30, mean(stats_ipredm[count<30,]), stats_roll)),
         total_roll = ifelse(is.na(total_roll) & count > 1230, mean(total_scorem[count>1230,]), ifelse(is.na(total_roll) & count < 30, mean(total_scorem[count<30,]), total_roll))
                              )

# create rolling average for effect CI order
crispectest <- crispectest[order(crispectest$count2),]

# mean replacement, then rolling average then replace tails w/ mean
crispectest <- crispectest %>%
  mutate(belief_roll2 = zoo::rollmean(belief_ipredm, k = 30, fill = NA),
         pro_roll2 = zoo::rollmean(pro_immigrantm, k = 30, fill = NA),
         topic_roll2 = zoo::rollmean(topic_ipredm, k = 30, fill = NA),
         stats_roll2 = zoo::rollmean(stats_ipredm, k = 30, fill = NA),
         total_roll2 = zoo::rollmean(total_scorem, k = 30, fill = NA),         
         belief_roll2 = ifelse(is.na(belief_roll2), mean(belief_ipredm[count2>1230,]), ifelse(is.na(belief_roll2) & count2 < 30, mean(belief_ipredm[count2<30,]), belief_roll2)),
         pro_roll2 = ifelse(is.na(pro_roll2) & count2 > 1230, mean(pro_immigrantm[count2>1230,]), ifelse(is.na(pro_roll2) & count2 < 30, mean(pro_immigrantm[count2 < 30,]), pro_roll2)),
         topic_roll2 = ifelse(is.na(topic_roll2) & count2 > 1230, mean(topic_ipredm[count2>1230,]), ifelse(is.na(topic_roll2) & count2 < 30, mean(topic_ipredm[count2<30,]), topic_roll2)),
         stats_roll2 = ifelse(is.na(stats_roll2) & count2 > 1230, mean(stats_ipredm[count2>1215,]), ifelse(is.na(stats_roll2) & count2 < 30, mean(stats_ipredm[count2<30,]), stats_roll2)),
         total_roll2 = ifelse(is.na(total_roll2) & count2 > 1230, mean(total_scorem[count2>1230,]), ifelse(is.na(total_roll2) & count2 < 30, mean(total_scorem[count2<30,]), total_roll2))
                              )
labs_y <- c("Model Score","Stats-Skill","Topic\nKnowledge","Pro Immigrant","Belief Hyp. True")

g3 <- ggplot(crispectest) +
  geom_tile(aes(x = count, y = 5, fill = belief_roll), height = 0.5) +
  geom_tile(aes(x = count, y = 4, fill = pro_roll), height = 0.5) +
  geom_tile(aes(x = count, y = 3, fill = topic_roll), height = 0.5) +
  geom_tile(aes(x = count, y = 2, fill = stats_roll), height = 0.5) +
  geom_tile(aes(x = count, y = 1, fill = total_roll), height = 0.5) +
  scale_y_continuous(breaks = c(1,2,3,4,5), labels=labs_y) +
    theme_classic() +
  xlab("Model") +
  theme(
    legend.position = "none",
    axis.title.y = element_blank()
  )
labs_y2 <- c("Model","l","e","t","e")


g4 <- ggplot(crispectest) +
  geom_tile(aes(x = count2, y = 5, fill = belief_roll2), height = 0.5) +
  geom_tile(aes(x = count2, y = 4, fill = pro_roll2), height = 0.5) +
  geom_tile(aes(x = count2, y = 3, fill = topic_roll2), height = 0.5) +
  geom_tile(aes(x = count2, y = 2, fill = stats_roll2), height = 0.5) +
  geom_tile(aes(x = count2, y = 1, fill = total_roll2), height = 0.5) +
  labs(fill = "Score") +
  scale_y_continuous(breaks = c(1,2,3,4,5), labels=labs_y2) +
  xlab("Model") +
    theme_classic() +
  theme(
    axis.text.y = element_text(color = "NA"),
    axis.title.y = element_blank(),
    legend.text = element_text(size = 6),
    legend.title = element_text(size = 8)
  )
```

### Fig 1. Combined New

```{r ggplot}
agg_png(filename = "results/Fig1_new.png", res = 144, height = 900, width = 1700)
ggarrange(g1,g2,g3,g4)
dev.off()

knitr::include_graphics("results/Fig1_new.png")
```

### Fig 2. Team-Level New
```{r fig2 new}

```




```{r corrs_clusters}
test_cor <- select(cri, cluster, AME_Z, AME, p, Hsupport, u_teamid)
test_cor <- test_cor %>%
  mutate(clus01 = ifelse(cluster == 1, 1, 0),
         clus02 = ifelse(cluster == 2, 1, 0),
         clus03 = ifelse(cluster == 3, 1, 0),
         clus04 = ifelse(cluster == 4, 1, 0),
         clus05 = ifelse(cluster == 5, 1, 0),
         clus06 = ifelse(cluster == 6, 1, 0),
         clus07 = ifelse(cluster == 7, 1, 0),
         clus08 = ifelse(cluster == 8, 1, 0),
         clus09 = ifelse(cluster == 9, 1, 0),
         clus10 = ifelse(cluster == 10, 1, 0))

cor <- cor(test_cor, use = "pairwise.complete")
```

We see that there are a few clusters that are more likely to have effects in a certain direction, more likely to have higher p-values or more likely to have subjective conclusions go in support.

#### Cluster descriptives

```{r clus_des}
cri_clusters <- aggregate(cri, by = list(cri$cluster), FUN = "mean")

cri_clusters_tbl <- t(select(cri_clusters, Jobs:countries, u_teamid))

kable_styling(kable(cri_clusters_tbl[1:30,], digits = 2))
```

#### Fig 2. Model Features Pooled

```{r}
crispectest2 <- crispectest %>%
  mutate(IMMIGRATION_MEASURE = iv_type,
         DV_MEASURE = dv_type)

crispectest2 <- subset(crispectest2, !is.na(crispectest2$IMMIGRATION_MEASURE))
crispectest2 <- subset(crispectest2, !is.na(crispectest2$DV_MEASURE))

crispectest2 <- select(crispectest2, IMMIGRATION_MEASURE, DV_MEASURE, est, lb, ub)

# remove/adjust attributes
  crispectest2 <- crispectest2[names(crispectest2)]
  choices <- 1:2
  attr(crispectest2, "choices") <- choices
  rownames(crispectest2) <- NULL
png(file = "results/Fig2_AME.png", width = 1200, height = 850, res = 144)

plot_rdf_spec_curve(crispectest2, "est", "lb", "ub", est_color = "grey", est_color_signeg = "red", est_color_sigpos = "blue", lower_to_upper = 1.5, est_label = "Average Marginal Effects\n(standardized)", choice_ind_point = F, pt_size = 0.3, ribbon = T)
dev.off()

knitr::include_graphics("results/Fig2_AME.png")
```
#### Fig 2_Stock. Model Features Stock Models Only

```{r fig2_stock}
crispectest3 <- subset(crispectest2, IMMIGRATION_MEASURE == "Stock")
crispectest3 <- select(crispectest3, -c(IMMIGRATION_MEASURE))

# remove/adjust attributes
  crispectest3 <- crispectest3[names(crispectest3)]
  choices <- 1
  attr(crispectest3, "choices") <- choices
  rownames(crispectest3) <- NULL
png(file = "results/Fig2_Stock_AME.png", width = 800, height = 650, res = 144)

plot_rdf_spec_curve(crispectest3, "est", "lb", "ub", est_color = "grey", est_color_signeg = "red", est_color_sigpos = "blue", lower_to_upper = 1.5, est_label = "Average Marginal Effects\n(standardized)", choice_ind_point = F, pt_size = 0.3, ribbon = T)
dev.off()

knitr::include_graphics("results/Fig2_Stock_AME.png")
```


#### Fig 2_Stock. Model Features Flow Models Only

```{r fig2_flow}
crispectest4 <- subset(crispectest2, IMMIGRATION_MEASURE == "Flow")
crispectest4 <- select(crispectest4, -c(IMMIGRATION_MEASURE))

# remove/adjust attributes
  crispectest4 <- crispectest4[names(crispectest4)]
  choices <- 1
  attr(crispectest4, "choices") <- choices
  rownames(crispectest4) <- NULL
png(file = "results/Fig2_Flow_AME.png", width = 800, height = 650, res = 144)

plot_rdf_spec_curve(crispectest3, "est", "lb", "ub", est_color = "grey", est_color_signeg = "red", est_color_sigpos = "blue", lower_to_upper = 1.5, est_label = "Average Marginal Effects\n(standardized)", choice_ind_point = F, pt_size = 0.3, ribbon = T)
dev.off()

knitr::include_graphics("results/Fig2_Flow_AME.png")
```

#### Fig 3. Sample Features

```{r fig3}
crispectest5 <- select(crispectest, SURVEY_WAVES, COUNTRIES, est, lb, ub)

crispectest5 <- subset(crispectest5, !is.na(crispectest5$SURVEY_WAVES))
crispectest5 <- subset(crispectest5, !is.na(crispectest5$COUNTRIES))


# remove/adjust attributes
  crispectest5 <- crispectest5[names(crispectest5)]
  choices <- 1:2
  attr(crispectest5, "choices") <- choices
  rownames(crispectest5) <- NULL
png(file = "results/Fig3_AME.png", width = 1200, height = 850, res = 144)

plot_rdf_spec_curve(crispectest5, "est", "lb", "ub", est_color = "grey", est_color_signeg = "red", est_color_sigpos = "blue", lower_to_upper = 1.5, est_label = "Average Marginal Effects\n(standardized)", choice_ind_point = F, pt_size = 0.3, ribbon = T)
dev.off()

knitr::include_graphics("results/Fig3_AME.png")
```

### Subjective Team Conclusions

#### Fig 4. By Researcher Qualities and 

```{r fig4_DV2, echo=F}


spec_team <- select(cri_team, BELIEF_HYPOTHESIS, PRO_IMMIGRANT, TOPIC_KNOWLEDGE, STATISTICS_SKILL, MODEL_SCORE, est, lb, ub)

spec_team <- subset(spec_team, !is.na(BELIEF_HYPOTHESIS))
spec_team <- subset(spec_team, !is.na(MODEL_SCORE))
# one problem with rdfanalysis is that it outputs the categories in the order they appear in the data, therefore we have to generate 3 rows of fake data to get the order right
  
fake <- as.data.frame(matrix(nrow = 3, ncol = 8))
fake[1,] <- c("Low","Low","Low","Low","Low",0,-0.01,0.01)
fake[2,] <- c("Mid","Mid","Mid","Mid","Mid",0,-0.01,0.01)
fake[3,] <- c("High","High","High","High","High",0,-0.01,0.01)
colnames(fake) <- c("BELIEF_HYPOTHESIS", "PRO_IMMIGRANT", "TOPIC_KNOWLEDGE", "STATISTICS_SKILL", "MODEL_SCORE", "est", "lb", "ub")
fake$est <- as.numeric(fake$est)
fake$ub <- as.numeric(fake$ub)
fake$lb <- as.numeric(fake$lb)

spec_team1 <- bind_rows(fake,spec_team)

# remove/adjust attributes
  spec_team1 <- spec_team1[names(spec_team1)]
  choices <- 1:5
  attr(spec_team1, "choices") <- choices
  rownames(spec_team1) <- NULL
png(file = "results/Fig4_AME.png", width = 700, height = 700, res = 144)

plot_rdf_spec_curve(spec_team1, "est", "lb", "ub", est_color = "grey", est_color_signeg = "red", est_color_sigpos = "blue", lower_to_upper = 1.5, est_label = "Subjective Conclusion", choice_ind_point = F, pt_size = 0.5, ribbon = T)
dev.off()

knitr::include_graphics("results/Fig4_AME.png")
```


#### Figure 5. (same as Fig 2 at team level)

#### Figure 6. (same as Fig 3 at team level)